---
layout: post
title: "Awesome Papers: 2017-02-1"
description: "Some papers about reinforcement learning"
category: "weekly"
tags: []
image: assets/images/weekly201702-1.jpg
---
### On The Construction of Extreme Learning Machine for Online and Offline One-Class Classification - An Expanded Toolbox 

*Chandan Gautam, Aruna Tiwari and Qian Leng* 

One-Class Classification (OCC) has been prime concern for researchers and 
effectively employed in various disciplines. But, traditional methods based 
one-class classifiers are very time consuming due to its iterative process and 
various parameters tuning. In this paper, we present six OCC methods based on 
extreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed 
classifiers mainly lie in two categories: reconstruction based and boundary 
based, which supports both types of learning viz., online and offline learning. 
Out of various proposed methods, four are offline and remaining two are online 
methods. Out of four offline methods, two methods perform random feature 
mapping and two methods perform kernel feature mapping. Kernel feature mapping 
based approaches have been tested with RBF kernel and online version of 
one-class classifiers are tested with both types of nodes viz., additive and 
RBF. It is well known fact that threshold decision is a crucial factor in case 
of OCC, so, three different threshold deciding criteria have been employed so 
far and analyses the effectiveness of one threshold deciding criteria over 
another. Further, these methods are tested on two artificial datasets to check 
there boundary construction capability and on eight benchmark datasets from 
different discipline to evaluate the performance of the classifiers. Our 
proposed classifiers exhibit better performance compared to ten traditional 
one-class classifiers and ELM based two one-class classifiers. Through proposed 
one-class classifiers, we intend to expand the functionality of the most used 
toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with 
all the present features of the toolbox. 

---

### Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks 

*Lars Mescheder, Sebastian Nowozin and Andreas Geiger* 

Variational Autoencoders (VAEs) are expressive latent variable models that 
can be used to learn complex probability distributions from training data. 
However, the quality of the resulting model crucially relies on the 
expressiveness of the inference model used during training. We introduce 
Adversarial Variational Bayes (AVB), a technique for training Variational 
Autoencoders with arbitrarily expressive inference models. We achieve this by 
introducing an auxiliary discriminative network that allows to rephrase the 
maximum-likelihood-problem as a two-player game, hence establishing a 
principled connection between VAEs and Generative Adversarial Networks (GANs). 
We show that in the nonparametric limit our method yields an exact 
maximum-likelihood assignment for the parameters of the generative model, as 
well as the exact posterior distribution over the latent variables given an 
observation. Contrary to competing approaches which combine VAEs with GANs, our 
approach has a clear theoretical justification, retains most advantages of 
standard Variational Autoencoders and is easy to implement. 

---

### Agent-Agnostic Human-in-the-Loop Reinforcement Learning 

*David Abel, John Salvatier, Andreas Stuhlm\"uller, Owain Evans* 

Providing Reinforcement Learning agents with expert advice can dramatically 
improve various aspects of learning. Prior work has developed teaching 
protocols that enable agents to learn efficiently in complex environments; many 
of these methods tailor the teacher's guidance to agents with a particular 
representation or underlying learning scheme, offering effective but 
specialized teaching procedures. In this work, we explore protocol programs, an 
agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is 
to incorporate the beneficial properties of a human teacher into Reinforcement 
Learning without making strong assumptions about the inner workings of the 
agent. We show how to represent existing approaches such as action pruning, 
reward shaping, and training in simulation as special cases of our schema and 
conduct preliminary experiments on simple domains. 

---

### Near Optimal Behavior via Approximate State Abstraction 

*David Abel, D. Ellis Hershkowitz, Michael L. Littman* 

The combinatorial explosion that plagues planning and reinforcement learning 
(RL) algorithms can be moderated using state abstraction. Prohibitively large 
task representations can be condensed such that essential information is 
preserved, and consequently, solutions are tractably computable. However, exact 
abstractions, which treat only fully-identical situations as equivalent, fail 
to present opportunities for abstraction in environments where no two 
situations are exactly alike. In this work, we investigate approximate state 
abstractions, which treat nearly-identical situations as equivalent. We present 
theoretical guarantees of the quality of behaviors derived from four types of 
approximate abstractions. Additionally, we empirically demonstrate that 
approximate abstractions lead to reduction in task complexity and bounded loss 
of optimality of behavior in a variety of environments. 

---

### Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks 

*Vahid Behzadan and Arslan Munir* 

Deep learning classifiers are known to be inherently vulnerable to 
manipulation by intentionally perturbed inputs, named adversarial examples. In 
this work, we establish that reinforcement learning techniques based on Deep 
Q-Networks (DQNs) are also vulnerable to adversarial input perturbations, and 
verify the transferability of adversarial examples across different DQN models. 
Furthermore, we present a novel class of attacks based on this vulnerability 
that enable policy manipulation and induction in the learning process of DQNs. 
We propose an attack mechanism that exploits the transferability of adversarial 
examples to implement policy induction attacks on DQNs, and demonstrate its 
efficacy and impact through experimental study of a game-learning scenario. 

---

### A Threshold-based Scheme for Reinforcement Learning in Neural Networks

*Thomas H. Ward*

A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.

---

### Building Machines That Learn and Think Like People

*Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, Samuel J. Gershman*

Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.

---

### The Predictron: End-To-End Learning and Planning

*David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, Thomas Degris*

One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.

---

### Learning what to look in chest X-rays with a recurrent visual attention model 

*Petros-Pavlos Ypsilantis and Giovanni Montana*

X-rays are commonly performed imaging tests that use small amounts of 
radiation to produce pictures of the organs, tissues, and bones of the body. 
X-rays of the chest are used to detect abnormalities or diseases of the 
airways, blood vessels, bones, heart, and lungs. In this work we present a 
stochastic attention-based model that is capable of learning what regions 
within a chest X-ray scan should be visually explored in order to conclude that 
the scan contains a specific radiological abnormality. The proposed model is a 
recurrent neural network (RNN) that learns to sequentially sample the entire 
X-ray and focus only on informative areas that are likely to contain the 
relevant information. We report on experiments carried out with more than 
100,000 X-rays containing enlarged hearts or medical devices. The model has 
been trained using reinforcement learning methods to learn task-specific 
policies. 

---

### Learning to reinforcement learn

*Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,Charles Blundell, Dharshan Kumaran, Matt Botvinick*

In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.
